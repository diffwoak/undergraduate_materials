## 全景图拼接

### 实验内容

1. 使用 Harris 角点检测器寻找关键点
2. 构建描述算子来描述图中的每个关键点，比较两幅图像的两组描述子，并进行匹配
3. 根据一组匹配关键点，使用 RANSAC 进行仿射变换矩阵的计算
4. 将第二幅图变换过来并覆盖在第一幅图上，拼接形成一个全景图像
5. 实现不同的描述子，并得到不同的拼接结果

### 实验过程

#### 一、Harris角点算法

##### Harris焦点算法原理

Harris角点检测算法通过计算图像中每个像素点的水平梯度$$ I_x $$和垂直梯度 $$ I_y $$ ，在计算每个像素点局部窗口的结构矩阵$$M$$，定义如下：

$$ M = \sum_{x,y} w(x, y) \begin{bmatrix} I_x^2 & I_x I_y \\ I_x I_y & I_y^2 \end{bmatrix} $$

其中$$ w(x, y) $$ 通常为高斯函数，加权邻域内的像素。通过结构矩阵M计算出每个像素点的角点响应函数：

$$ R = \text{det}(M) - k \cdot \text{trace}^2(M) $$

其中$$ \text{det}(M) $$ 是$$M$$的行列式，$$ \text{trace}(M) $$ 是$$M$$的迹，$$k$$是常数，通常取0.04~0.06。比较每个像素点的角点响应函数值，角点 $$|R|$$ 很大，平坦的区域 $$|R|$$ 很小，边缘的 $$|R|$$  为负值。

##### 代码实现

计算梯度，得到$$M=\begin{bmatrix} A & C \\ C & B \end{bmatrix}$$，$$det(M)=AC-B^2$$,$$trace(M)=A+C$$,则$$R=AC-B^2-k(A+C)$$

```python
def calculate_harris(im, ksize = 3, k = 0.06):
	# compute Ix Iy
    Ix = cv2.Sobel(im, cv2.CV_64F, 1, 0, ksize)
    Iy = cv2.Sobel(im, cv2.CV_64F, 0, 1, ksize)
	# compute A B C
    A = cv2.GaussianBlur(Ix * Ix,(ksize,ksize),0.5,0.5)
    C = cv2.GaussianBlur(Ix * Iy,(ksize,ksize),0.5,0.5)
    B = cv2.GaussianBlur(Iy * Iy,(ksize,ksize),0.5,0.5)
	
    det = A * B - C ** 2
    tr = A + B
    return det - k*tr
```

为防止角点密集，会对角点响应函数进行阈值处理和非极大值抑制

```python
def select_points(harrisim, min_dist = 10, threshold = 0.2):
	# 阈值处理
	harris_t1 = (harrisim > harrisim.max() * threshold) * 1
    
	coords = array(harris_t1.nonzero()).T
	values_t1 = [harrisim[c[0], c[1]] for c in coords]
    # 按筛选点的R值顺序，对该点领域非极大值抑制，即邻域内不存在其他R值更低的角点
	index = argsort(values_t1)
	allowed_locations = zeros(harrisim.shape)
	allowed_locations[min_dist: -min_dist, min_dist: -min_dist] = 1
	harris_t2 = []
	for i in index:
		if allowed_locations[coords[i, 0], coords[i, 1]] == 1:
			harris_t2.append(coords[i])
			allowed_locations[(coords[i, 0] - min_dist) : (coords[i, 0] + min_dist), (coords[i, 1] - min_dist) : (coords[i, 1] + min_dist)] = 0
	return harris_t2
```

绘制图像

```python
def draw_with_points(image, coords,output_path):
	# 绘制点到图像上
	for c in coords:
		cv2.circle(image, (int(c[1]), int(c[0])), 2, (0, 0, 255), -1) 
	image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
	# 保存图像
	cv2.imwrite(output_path, image)
	# 显示图像
	cv2.imshow('Image with Points', image)
	cv2.waitKey(0)
	cv2.destroyAllWindows()
```

主函数

```python
if __name__=="__main__":
	input_path = 'images/sudoku.png'
	output_path = 'results/sudoku_keypoints.png'
	im = cv2.imread(input_path)
	img = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)
	harrisim = calculate_harris(img)
	coords = select_points(harrisim, 10)
	draw_with_points(im, coords,output_path)
```

##### 实验结果

对 images/目录下的 sudoku.png 图像进行角点检测（适当进行后处理），输出对应的角点检测结果

<img src="C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\sudoku_keypoints-17140195964921.png" alt="sudoku_keypoints" style="zoom:67%;" />

#### 二、关键点描述与匹配

##### 1. Harris应用检测

使用Harris角点算法对下面二副图像进行关键点检测，结果如下：

<img src="C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\uttower1_keypoints-17140196575053.jpg" alt="uttower1_keypoints" style="zoom:67%;" />

<img src="C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\uttower2_keypoints-17140196640015.jpg" alt="uttower2_keypoints" style="zoom:67%;" />

##### 2. SIFT和HOG特征描述子

分别使用 SIFT 特征和 HOG 特征作为描述子获得两幅图像的关键点的特征，使用欧几里得距离作为特征之间相似度的度量，并绘制两幅图像之间的关键点匹配的情况

###### 代码实现

计算HOG特征

```python
def get_hogs(img_grey,coords):
	# 创建HOG描述符
	winSize = (16, 16)
	blockSize = (8, 8)
	blockStride = (4, 4)
	cellSize = (4, 4)
	nbins = 9
	hog = cv2.HOGDescriptor(winSize, blockSize, blockStride, cellSize, nbins)
	# 提取HOG特征
	hogs = []
	for [x,y] in coords:
		roi = img_grey[x - winSize[0] // 2:x + winSize[0] // 2, y - winSize[1] // 2:y + winSize[1] // 2]
		if roi.shape[0] != winSize[0] or roi.shape[1] != winSize[1]:
			continue
		h = hog.compute(roi)
		hogs.append(h)
	return hogs
```

计算 SIFT 特征，``cv2.SIFT_create``计算关键点及特征，为了使用harris的关键点，直接查找距离SIFT特征点最近的作为特征值

```python
def get_sifts(img_grey,coords):
	# 创建SIFT对象
	sift = cv2.SIFT_create()
	# 提取SIFT特征
	kp, des = sift.detectAndCompute(img_grey, None)
	# 提取角点对应的SIFT特征
	sifts = []
	for [x,y] in coords:
		# 在SIFT特征中查找距离最近的关键点
		closest_kp = min(kp, key=lambda k: ((k.pt[0] - x) ** 2 + (k.pt[1] - y) ** 2) ** 0.5)
		# 获取该关键点的特征描述符
		index = kp.index(closest_kp)
		sifts.append(des[index])
	return sifts
```

通过欧氏距离匹配两幅图像的关键点

```python
def match_descriptors(des1, des2, threshold=0.8):
    matches = []
    for i, d1 in enumerate(des1):
        best_index = -1
        best_distance = float('inf')
        second_distance = float('inf')
        for j, d2 in enumerate(des2):
            distance = np.linalg.norm(d1 - d2)  # 计算欧氏距离
            if distance < best_distance:
                second_distance = best_distance
                best_distance = distance
                best_index = j
            elif distance < second_distance:
                second_distance = distance
        # 检查最佳距离和次佳距离之间的比值是否小于阈值
        if best_distance < threshold * second_distance:
            matches.append((i, best_index))
    return matches
```

通过``cv2.drawMatches``显示匹配结果

```python
def show_match(img1,img2,coord1,coord2,des1,des2,out_path=None):
	matches = match_descriptors(des1, des2)
	kp1 = [cv2.KeyPoint(float(y), float(x), 1) for (x, y) in coord1]
	kp2 = [cv2.KeyPoint(float(y), float(x), 1) for (x, y) in coord2]
	match_img = cv2.drawMatches(img1, kp1, img2, kp2, [cv2.DMatch(_[0], _[1], 0) for _ in matches], None)
	match_img = cv2.cvtColor(match_img, cv2.COLOR_BGR2RGB)
	if out_path != None:
		cv2.imwrite(out_path, match_img)
	cv2.imshow('Matches', match_img)
	cv2.waitKey(0)
	cv2.destroyAllWindows()
```

###### 实验结果

HOG特征匹配结果

<img src="C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\uttower_match.png" alt="uttower_match" style="zoom:67%;" />

SIFT特征匹配结果

![uttower_match](C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\uttower_match-17140199317918.png)

##### 3. RANSAC

使用 RANSAC 求解仿射变换矩阵，实现图像的拼接，分析对比 SIFT 特征和 HOG 特征在关键点匹配过程中的差异

###### 代码实现

```python
def Panorama_uttower():
	# hog or sift
	hog_sift = 'hog'
	img1, coords_1, des_1 = get_pic_info('images/uttower1.jpg',hog_sift)
	img2, coords_2, des_2 = get_pic_info('images/uttower2.jpg',hog_sift)
   
	h1, w1 = img1.shape[:2]
	h2, w2 = img2.shape[:2]
	matches = match_descriptors(des_1, des_2)
	kp1 = np.array([(coords_1[m[0]][1],coords_1[m[0]][0]) for m in matches],dtype=np.float32)
	kp2 = np.array([(coords_2[m[1]][1],coords_2[m[1]][0]) for m in matches],dtype=np.float32)
	# RANSAC求解变换矩阵
	affine_matrix, _ = cv2.findHomography(kp2, kp1, cv2.RANSAC, ransacReprojThreshold=3)
    # 图像2变换拼接到图像1
	panorama = cv2.warpPerspective(img2, affine_matrix, (w1+w2, h1))
	panorama[0:h1, 0:w1] = img1
	panorama = cv2.cvtColor(panorama, cv2.COLOR_BGR2RGB)
    # 保存图像
	if hog_sift == 'hog':
		cv2.imwrite("results/uttower_stitching_hog.png", panorama)
	else:
		cv2.imwrite("results/uttower_stitching_sift.png", panorama)
	# 显示全景图
	cv2.imshow('Panorama', panorama)
	cv2.waitKey(0)
	cv2.destroyAllWindows()
```

###### 实验结果

HOG特征结果

![uttower_stitching_hog](C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\uttower_stitching_hog.png)

SIFT特征结果

![uttower_stitching_sift](C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\uttower_stitching_sift.png)

根据harris角点匹配情况以及拼接结果，看出使用SIFT特征进行匹配的情况误差较大，但可以通过RANSAC 对噪声很好的鲁棒性去除错误匹配的影响，达成与HOG特征拼接相近的结果。

这里再尝试使用全部SIFT特征的关键点，利用RANSAC拼接结果如下，效果比前两个使用harris角点匹配结果略好,相对的运行耗时较长

![uttower_stitching_sift1](C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\uttower_stitching_sift1.png)

##### 4. 拼接多张图像

基于 SIFT + RANSAC 的拼接方法用到多张图像

###### 代码实现

根据变量``hog_sift``决定使用哪种特征进行匹配拼接，将代码改写为拼接4张图片，固定第3张图片，计算出图2和图4对图3的匹配点的投影矩阵，再通过图2投影后的关键点与图1匹配的关键点计算出图1的投影矩阵，最后照常拼接即可

```python
def Panorama_yosemite(hog_sift = None):
	img1, coords_1, des_1 = get_pic_info('images/yosemite1.jpg',hog_sift)
	img2, coords_2, des_2 = get_pic_info('images/yosemite2.jpg',hog_sift)
	img3, coords_3, des_3 = get_pic_info('images/yosemite3.jpg',hog_sift)
	img4, coords_4, des_4 = get_pic_info('images/yosemite4.jpg',hog_sift)
	h1, w1 = img1.shape[:2]
	h2, w2 = img2.shape[:2]
	h3, w3 = img3.shape[:2]
	h4, w4 = img4.shape[:2]
	# hog or sift or None(use all sift feature for matching)
	if hog_sift != None:
		matches_12 = match_descriptors(des_1, des_2,0.85)
		matches_23 = match_descriptors(des_2, des_3,0.85)
		matches_34 = match_descriptors(des_3, des_4,0.85)
		# 将第3张图片固定
		kp12_1 = np.array([(coords_1[m[0]][1],coords_1[m[0]][0]) for m in matches_12],dtype=np.float32)
		kp12_2 = np.array([(coords_2[m[1]][1],coords_2[m[1]][0]) for m in matches_12],dtype=np.float32)
		kp23_2 = np.array([(coords_2[m[0]][1],coords_2[m[0]][0]) for m in matches_23],dtype=np.float32)
		kp23_3 = np.array([(coords_3[m[1]][1]+w1+w2,coords_3[m[1]][0]) for m in matches_23],dtype=np.float32)
		kp34_3 = np.array([(coords_3[m[0]][1]+w1+w2,coords_3[m[0]][0]) for m in matches_34],dtype=np.float32)
		kp34_4 = np.array([(coords_4[m[1]][1],coords_4[m[1]][0]) for m in matches_34],dtype=np.float32)
	else:
		sift = cv2.SIFT_create()
		kp_1, des_1 = sift.detectAndCompute(img1, None)
		kp_2, des_2 = sift.detectAndCompute(img2, None)
		kp_3, des_3 = sift.detectAndCompute(img3, None)
		kp_4, des_4 = sift.detectAndCompute(img4, None)
		matches_12 = match_descriptors(des_1, des_2,0.9)
		matches_23 = match_descriptors(des_2, des_3,0.9)
		matches_34 = match_descriptors(des_3, des_4,0.9)
		kp12_1 = np.array([kp_1[m[0]].pt for m in matches_12],dtype=np.float32)
		kp12_2 = np.array([kp_2[m[1]].pt for m in matches_12],dtype=np.float32)
		kp23_2 = np.array([kp_2[m[0]].pt for m in matches_23],dtype=np.float32)
		kp23_3 = np.array([(kp_3[m[1]].pt[0]+w1+w2,kp_3[m[1]].pt[1]) for m in matches_23],dtype=np.float32)
		kp34_3 = np.array([(kp_3[m[0]].pt[0]+w1+w2,kp_3[m[0]].pt[1]) for m in matches_34],dtype=np.float32)
		kp34_4 = np.array([kp_4[m[1]].pt for m in matches_34],dtype=np.float32)
	
	# 计算图二图四的变换矩阵
	affine_matrix_2, _ = cv2.findHomography(kp23_2, kp23_3, cv2.RANSAC, ransacReprojThreshold=3)
	affine_matrix_4, _ = cv2.findHomography(kp34_4, kp34_3, cv2.RANSAC, ransacReprojThreshold=3)
	# 先计算出图二的变换矩阵,得到图二关键点转换后的位置,进而得到图一的变换矩阵
	kp12_2 = np.hstack((kp12_2, np.ones((len(kp12_2), 1))))
	transform_kp12_2 = np.dot(affine_matrix_2, kp12_2.T).T
	kp12_2 = transform_kp12_2[:, :2] / transform_kp12_2[:, 2:]
	affine_matrix_1, _ = cv2.findHomography(kp12_1, kp12_2, cv2.RANSAC, ransacReprojThreshold=3)
	# 拼接
	panorama1 = cv2.warpPerspective(img1, affine_matrix_1, (w1+w2+w3+w4, h1+50))
	panorama2 = cv2.warpPerspective(img2, affine_matrix_2, (w1+w2+w3+w4, h1+50))
	panorama4 = cv2.warpPerspective(img4, affine_matrix_4, (w1+w2+w3+w4, h1+50))
	panorama4[panorama1 > 0] = panorama1[panorama1 > 0]
	panorama4[panorama2 > 0] = panorama2[panorama2 > 0]
	panorama4[0:h1, w1+w2:w1+w2+w3] = img3

	panorama4 = cv2.cvtColor(panorama4, cv2.COLOR_BGR2RGB)

	if hog_sift == 'hog':
		cv2.imwrite("results/yosemite_stitching_hog.png", panorama4)
	elif hog_sift == 'sift':
		cv2.imwrite("results/yosemite_stitching_sift.png", panorama4)
	else:
		cv2.imwrite("results/yosemite_stitching.png", panorama4)
```

###### 实验结果

SIFT特征结果

![yosemite_stitching_sift](C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\yosemite_stitching_sift.png)

其他：

HOG特征结果

![yosemite_stitching_hog](C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\yosemite_stitching_hog.png)

直接使用SIFT特征关键点结果

![yosemite_stitching](C:\Users\asus\Desktop\大三下\模式识别\模式识别课程第一次作业\全景图拼接.assets\yosemite_stitching.png)

对比分析三种结果，其中直接使用SIFT特征关键点结果的效果最稳定，但运行耗时较长，而上面通过Harris角点计算SIFT和HOG特征的两种方法效果受到调参波动较大，当计算欧氏距离匹配关键点时，受到阈值影响容易产生过少或较乱的匹配点，导致效果多变。



### 参考连接

Harris：

[计算机视觉基础-图像处理: Harris特征点检测 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/150411153)

[【理解】经典角点检测算法--Harris角点_harris角点检测算法-CSDN博客](https://blog.csdn.net/SESESssss/article/details/106774854)

[局部图像描述子——Harris角点检测器_harris角点检测描述子-CSDN博客](https://blog.csdn.net/weixin_42262128/article/details/107808511)

HOG：

[图像特征描述子（一）——HOG-CSDN博客](https://blog.csdn.net/u013972657/article/details/118294795)

[【特征检测】HOG特征算法_比较两幅图像相似性-基于hog特征-CSDN博客](https://blog.csdn.net/hujingshuang/article/details/47337707)

[计算机视觉基础：HOG特征描述算⼦ - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/156032691)

RANSAC：

[仿射变换Affine+RANSAC_基于ransac的仿射变换算法-CSDN博客](https://blog.csdn.net/He3he3he/article/details/98058520)

全文思路：[全景拼接|Harris角点检测，RANSAC以及HOG描述符完成 - 掘金 (juejin.cn)](https://juejin.cn/post/7074562849753792543)
