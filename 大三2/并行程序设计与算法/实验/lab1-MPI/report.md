## Lab1 - 基于MPI的并行矩阵乘法

### 实验要求

使用MPI点对点通信方式实现并行通用矩阵乘法，并实验分析不同进程数量、矩阵规模时的性能

输入：m, n, k 三个整数，每个整数取值范围均为[128, 2048]

问题描述：随机生成m×n的A矩阵及n×k的B矩阵，矩阵相乘得到C矩阵

要求：1. 调整记录不同进程数量（1\~16）及矩阵规模（128\~2048）的时间开销，分析性能

2. 讨论优化方向：a) 在内存有限情况下，如何进行大规模矩阵乘法；b)如何提高大规模稀疏矩阵乘法性能

### 实验过程

##### 1. 实现思路

对于多个进程，将0号进程作为主进程，负责读取参数，初始化矩阵，并将矩阵乘法的工作划分到其它进程，将矩阵A按行分割，以及整个B，发送到其他进程；子进程负责接收矩阵A的部分行和整个矩阵B，对二者进行矩阵乘法，得到结果即为矩阵C的对应部分行，再传送C回主进程。主进程收集每个子进程计算结果得到完整的矩阵C。

##### 2. 代码实现

**发送**：通过输入矩阵规模，初始化A、B随机矩阵，并计算划分每个进程负责计算的A矩阵行数``per_row``,以及需要事先将矩阵的规模传递给子进程，为减少发送次数，将矩阵规模的``k``、``n``以及实际负责的矩阵行数``last_row - first_row``存入``send_var``中发送到各个子进程。以下展示发送过程的主要代码：

```c
for (int i = 0; i < comm_sz - 1; i++) {
    first_row = i * per_row; last_row = min((i + 1) * per_row , m );
    send_var[0] = last_row - first_row;               
    MPI_Send(&send_var[0], 3, MPI_INT, i + 1, 0, MPI_COMM_WORLD);
    MPI_Send(&A[first_row * k], (last_row - first_row) * k, MPI_DOUBLE, i + 1, 1, MPI_COMM_WORLD);
    MPI_Send(&B_T[0], k * n,MPI_DOUBLE , i + 1, 2, MPI_COMM_WORLD);
}
```

**计算**：子进程负责矩阵计算，内容包括：①接收矩阵规模参数；②接收需要计算的部分A、B矩阵；③调用``mpi_multiple``（使用lab0实现的普通串行矩阵乘法）进行矩阵乘法，结果存入C矩阵；④发送该C矩阵回主进程，注意这里使用到的发送函数是``MPI_Ssend``，同步发送，防止一些进程提前发送数据到主进程发生阻塞以至死锁。

```c
int n, k,rows;
int* recv_var = (int*)malloc(4 * sizeof(int));
double* A_row, *B_T, *C;

MPI_Recv(&recv_var[0], 3, MPI_INT, 0, 0, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
rows = recv_var[0]; k = recv_var[1]; n = recv_var[2];
free(recv_var);
A_row = (double*)malloc(rows * k * sizeof(double));
B_T = (double*)malloc(n * k * sizeof(double));
C = (double*)malloc(rows * n * sizeof(double));

MPI_Recv(&A_row[0], rows * k, MPI_DOUBLE, 0, 1, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
MPI_Recv(&B_T[0], n * k, MPI_DOUBLE, 0, 2, MPI_COMM_WORLD, MPI_STATUS_IGNORE);

mpi_multiple(A_row, B_T, C, rows, k, n);

MPI_Ssend(&C[0], rows * n, MPI_DOUBLE, 0, 3, MPI_COMM_WORLD);
       
free(A_row);
free(B_T);
free(C);
```

**接收**：0号进程按顺序接收各个子进程的计算结果，存入C矩阵

```c
for (int i = 0; i < comm_sz - 1; i++) {
    first_row = i * per_row; last_row = min((i + 1) * per_row, m );
    MPI_Recv(&C[first_row * n], (last_row - first_row) * n, MPI_DOUBLE, i + 1, 3, MPI_COMM_WORLD, MPI_STATUS_IGNORE);
} 
```

##### 3. 其他

- 考虑矩阵乘法中B矩阵需要以列方向进行访问，在发送B矩阵到各进程前对B矩阵进行装置，如此便可在每个子进程计算中以行方向进行访问，利用了空间局部性
- 需要考虑情况：只指定使用一个进程时，则由0号进程负责全部计算

##### 4. 运行结果

```shell
// 编译
mpicc -g -Wall -o mpi MpiMatMulti.c
// 运行
mpiexec -n 4 ./mpi
```

<img src="https://gitee.com/e-year/images/raw/master/img/202403261635986.png" alt="image-20240326163546099" style="zoom:67%;" />

### 性能分析

统计不同进程数、不同矩阵规模（128即生成都为128×128的A、B矩阵），耗时单位毫秒。

| 进程数\矩阵规模 |   128    |   256    |    512    |   1024    |    2048    |
| :-------------: | :------: | :------: | :-------: | :-------: | :--------: |
|        1        |  14.864  | 202.803  | 1073.213  |  7985.93  | 60900.499  |
|        2        |  19.511  | 385.543  | 2312.017  | 16573.624 | 128205.601 |
|        4        |  93.707  | 293.323  | 1785.764  | 12599.842 | 91601.921  |
|        8        | 201.313  | 901.588  | 2598.775  | 13401.659 | 88503.306  |
|       16        | 5592.517 | 5097.548 | 12700.026 | 46498.046 | 228397.445 |

分析上表，因为进程数为1时无需进程通信，而进程数为2时实际计算工作也只在一个进程，因此2的耗时全部都长于1。比较进程数2和4的耗时，在矩阵规模为128时进程数为2仍占优势，随着矩阵规模的增长，进程数为4的耗时都要少于进程数为2的耗时。随着进程数的增加，由于通信耗时的增加，在矩阵规模不大时的耗时都明显长于进程数小的耗时，如进程数8的耗时在矩阵规模达到2048时才有优于进程数4的性能，进程数16的耗时直到2048的矩阵规模也无法取得优于较少进程数的性能。

### 讨论内容

##### 1. 在内存有限情况下，如何进行大规模矩阵乘法

对于大规模的矩阵乘法：可将矩阵划分进行分块计算，就是本次实验的实现思路，可进一步对矩阵B进行分块，不将一整个B矩阵发送到各个进程，减少通信量，考虑分步骤计算，先计算局部矩阵乘法，再将局部结果汇总计算最终结果；以及通过优化存储方式，使用稀疏矩阵存储格式，或将矩阵数据分布存储在多个节点上，降低单个节点的内存压力；以及可以考虑及时释放无用内存，合理选择数据类型等方式来减少内存占用

##### 2. 如何提高大规模稀疏矩阵乘法性能

对稀疏矩阵的存储结构优化，比如使用coo，csr，csc等存取格式，减少存储空间，提高访问效率；使用分块方式，对稀疏矩阵进行灵活划分，将数据较集中的区域划分成一个块进行计算；对于稀疏矩阵，统计矩阵中非零的元素位置存储，对于该行或该列的元素，只需搜索该行或该列需要计算的非零数进行计算加和即可。
